{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The previous notebook began to run a little more slowly than desirable, so off we just into another one.\n",
    "* Build a data modeling paradigm for practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "def beta_1_hat(dat):\n",
    "    x_m = x_mean(dat)\n",
    "    y_m = y_mean(dat)\n",
    "    beta_1_hat_num = sum([(x - x_m)*(y - y_m) for (x,y) in dat.iteritems()])\n",
    "    beta_1_hat_denom = sum([(d[0] - x_m)**2 for d in dat.iteritems()])\n",
    "    beta_1_hat = beta_1_hat_num / beta_1_hat_denom\n",
    "    return beta_1_hat\n",
    "\n",
    "def beta_0_hat(dat):\n",
    "    beta_0_hat = y_mean(dat) - beta_1_hat(dat)*x_mean(dat)\n",
    "    return beta_0_hat\n",
    "\n",
    "def n(dat):\n",
    "    return len(dat)\n",
    "\n",
    "def x_vals(dat):\n",
    "    return np.array(dat.index)\n",
    "\n",
    "def x_mean(dat):\n",
    "    return np.mean(x_vals(dat))\n",
    "\n",
    "def y_mean(dat):\n",
    "    return dat.mean()\n",
    "\n",
    "def sse(dat):\n",
    "    return sum([(d[1] - (beta_0_hat(dat) + beta_1_hat(dat)*d[0]))**2 for d in dat.iteritems()])\n",
    "\n",
    "def sst(dat):\n",
    "    return sum([(d[1] - y_mean(dat))**2 for d in dat.iteritems()])\n",
    "\n",
    "def r_squared(dat):\n",
    "    return 1 - sse(dat)/sst(dat)\n",
    "\n",
    "def variance(dat):\n",
    "    return sse(dat)/(n(dat) - 2)\n",
    "\n",
    "def standard_deviation_estimator(dat):\n",
    "    return variance(dat)**(1/2)\n",
    "\n",
    "def beta_1_hat_standard_deviation_estimator(dat):\n",
    "    return standard_deviation_estimator(dat)/math.sqrt(sum(x_vals(dat)**2) - (sum(x_vals(dat))**2)/n(dat))\n",
    "\n",
    "def beta_1_hat_confidence_interval(dat, confidence=.95):\n",
    "    t_rv = scipy.stats.t(n(dat)-2)\n",
    "    t_value = t_rv.ppf(1 - (1 - confidence)/2)\n",
    "    interval = (beta_1_hat(dat) - t_value * beta_1_hat_standard_deviation_estimator(dat),\n",
    "                beta_1_hat(dat) + t_value * beta_1_hat_standard_deviation_estimator(dat))\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = Series({16.1:4.41,\n",
    "              31.5:6.81,\n",
    "              21.5:5.26,\n",
    "              22.4:5.99,\n",
    "              20.5:5.92,\n",
    "              28.4:6.14, \n",
    "              30.3:6.84, \n",
    "              25.6:5.87, \n",
    "              32.7:7.03, \n",
    "              29.2:6.89, \n",
    "              34.7:7.87}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.275454545454545,\n",
       " 26.627272727272725,\n",
       " 2.2249345952600814,\n",
       " 0.15211921970685938,\n",
       " 0.88135230886566351,\n",
       " 0.01860448731760117,\n",
       " (0.11003294546222395, 0.19420549395149481))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean(dat), x_mean(dat), beta_0_hat(dat), beta_1_hat(dat), r_squared(dat), beta_1_hat_standard_deviation_estimator(dat), beta_1_hat_confidence_interval(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear regression model in which we have a high degree of confidence can then be used to construct an estimate of the true mean of the population (as opposed to the mean of the population: a population by itself does not provide enough information to construct a confidence interval).\n",
    "* $E(\\hat{\\beta}_0 + \\hat{\\beta}_1\\cdot x^*) = \\beta_0 + \\beta_1 x^*$ (duh)\n",
    "* $\\sigma^2(\\hat{\\beta}_0 + \\hat{\\beta}_1\\cdot x^*) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{n(x^* - \\bar{x})^2}{n\\sum x_i^2 - (\\sum x_i)^2}\\right)$.\n",
    "* Notice that variance increases as $x^*$ moves away from $\\bar{x}$! This implies that we are more sure of our fit near the mean and less sure of it further away, which, of course, makes sense: small differences in slope cause large differences in value at extremes.\n",
    "* $T = \\frac{\\hat{\\beta}_0 + \\hat{\\beta}_1\\cdot x^* - (\\beta_0 + \\beta_1\\cdot x^*)}{s_{\\hat{\\beta}_0 + \\hat{\\beta}_1\\cdot x^*}} \\sim t[n-2](\\cdot)$, and thus our confidence interval is $\\hat{\\beta}_0 + \\hat{\\beta}_1 x^* \\pm t_{\\alpha/2, n-2}\\cdot s_{\\hat{\\beta}_0 + \\hat{\\beta}_1\\cdot x^*}$.\n",
    "* A property of the normal that follows from the stochasticly useful **Bonferroni inequality** is that given that we are $\\alpha$ confident in one interval and $\\beta$ confident in another one, we are $1 - (1 - \\alpha + 1 - \\beta)$ confident in both being simultaneously true. For instance if we are 99% confident that the value for y(2) is distributed within (12,24) and 99% confident that the value of y(4) is distributed within (20, 30) then we are 98% confident in the statement that both y(2) and y(4) are respectively in their intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_x_variance(dat, estimated_x):\n",
    "    # This method returns significant accumulated floating-point error.\n",
    "    # Would be better to work with the logs of things, but I don't immediately know how to do that. Should learn that technique.\n",
    "    return standard_deviation_estimator(dat)*math.sqrt(1/n(dat) + n(dat)*(estimated_x - x_mean(dat))**2/(n(dat)*sum([d[0]**2 for d in dat.iteritems()]) - sum([d[0] for d in dat.iteritems()])**2))\n",
    "\n",
    "def estimated_x_interval(dat, estimated_x, confidence=.95):\n",
    "    t_rv = scipy.stats.t(n(dat)-2)\n",
    "    t_value = t_rv.ppf(1 - (1 - confidence)/2)\n",
    "    interval = (beta_0_hat(dat) + beta_1_hat(dat)*estimated_x - t_value * estimated_x_variance(dat, estimated_x),\n",
    "                beta_0_hat(dat) + beta_1_hat(dat)*estimated_x + t_value * estimated_x_variance(dat, estimated_x))\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = Series(\n",
    "    {1000:220,\n",
    "     1100:280,\n",
    "     1200:350,\n",
    "     1250:375,\n",
    "     1300:450,\n",
    "     1400:470,\n",
    "     1450:500}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322.49947180995491, 378.34559861258015)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_x_interval(dat, 1200, confidence=.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which solves example 12.14 on page 481!\n",
    "* At this point I went back and derived the non-obvious parts of what I'd read from principles.\n",
    " * http://stats.stackexchange.com/questions/76444/why-is-chi-square-used-when-creating-a-confidence-interval-for-the-variance\n",
    "* At this point I want to go back and rederive the linear modeling results so far, from first principles.\n",
    "* ...done..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **standardized residual plot** is incredibly useful for checking the rightness of a regression.\n",
    "* A distribution is **intrinsically linear** if it can, by some transformation, be made linear (and thus, regressable).\n",
    "* Does that mean lognormal and other such transformable variables are intrinsically normal?\n",
    "* Polynomial regression is usually only appropriate for up to a cubic.\n",
    "* You can technically get 100% accuracy with n - 1 degrees of a polynomial regression.\n",
    "* Again there is a formula for $r^2$, coefficient of multiple determination.\n",
    "* A higher-degree regression will include lower-degree ones as special cases, and so will be strictly superior. So it will have a higher $r^2$, but the complexity, will mean that it might well be worse anyway!\n",
    "* There is also an adjusted coefficient which weighs which is useful for weighing a higher-order regression against a lower one. If the adjusted $r^2$ compares poorly against the basic $r^2$ of the lower-polynomial model, then the lower model should be used, as not enough benefit is provided in the additional degree.\n",
    "* How is the adjustment made, logically? Dunno-figure it out if I need it.\n",
    "* Since with enough observations the regression error will be approximately normal, you can compute the boundaries against the polynomial approximation in the usual way. This nets you $\\hat{\\beta}_i \\pm t_{\\alpha/2,n-(k+1)}\\cdot s_{\\hat{\\beta}_i}$ (where n is the number of observations and k the number of degrees).\n",
    "* As with the linear model, there are also confidence intervals for the mean and for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Multiple regression analysis** relates a single dependent variable $Y$ to several independent (predictor) values $x_k$. \n",
    "* So basically, multivariate regression.\n",
    "* These models can be typified as having interaction (in which case you have $\\beta_1 x_1 x_2$ type stuff) or no interaction (in which case all of the $x_1$ and $x_2$ terms are strictly seperate). The overall model looks something like $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$\n",
    "* Again there is a confidence interval construction and a hypothesis test.\n",
    "* Skipping \"Other issues\"..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
