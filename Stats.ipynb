{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics for Engineering and the Sciences\n",
    "\n",
    "I've taken courses in business statistics (bleh), probability (much better!), and stochastic processes (fascinating and difficult and fascinating), now I'm reading this book to to start off with stapling all of my knowledge together. Jupyter notebooks are ideal because they're laptop-on-my-lap-on-the-train compatible.\n",
    "\n",
    "Basically this is me self-studying Statistics II. After this I'm reading a Bayesian book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A **trimmed mean** removes some lowest and highest percentile of the data (outlier correction). The median can be thought of as a competely trimmed mean.\n",
    "* Mean includes full weight of outliers, median takes zero stock. Sometimes you want a measure somewhere in between, which is when a trimmed mean is useful (more analytical than simply \"discard these five values as being too outlier-y).\n",
    "* **Box plot** bits:\n",
    " * **First IQR** and **third IQR** are lower-half and upper-half medians, respectively, these are the bottom and top of a box plot respectively. The distance between the two is **fourth spread**, $f_4$, and this is a statistic that is resistant to outliers.\n",
    " * Line down the middle is the median (not mode!).\n",
    " * Lines extend on either side to the smallest value still within $1.5 \\times f_4$  of the median.\n",
    " * **Outliers** are in the $\\pm [1.5 f_4, 3 f_4 ]$ range, **extreme outliers** are in the $\\geq 3 f_4$ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box plot in Plotly in Jupyter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import plotly.tools as tls\n",
    "\n",
    "# Set my plotly credentials.\n",
    "data = json.load(open('plotly_credentials.json'))['credentials']\n",
    "tls.set_credentials_file(username=data['username'], api_key=data['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfuly sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~ResidentMario/0 or inside your plot.ly account where it is named 'pandas-box-plot'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~ResidentMario/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Populate a pandas DataFrame with randomized letter values.\n",
    "# It'd be more sensible to get letter frequency -grams but /\\o/\\ this is an example direct from the source.\n",
    "N = 100\n",
    "y_vals = {}\n",
    "for letter in list(string.ascii_uppercase):\n",
    "    # np.random.randn() returns N random standard normal samples. Defaults to five.\n",
    "    # Note, numpy doesn't provide sigma and mu parameters, you have to do the normal transform yourself. Cute!\n",
    "    y_vals[letter] = np.random.randn(N)+(3*np.random.randn())\n",
    "\n",
    "df = pd.DataFrame(y_vals)\n",
    "# df.head returns the first five rows of the DataFrame.\n",
    "df.head()\n",
    "\n",
    "data = []\n",
    "\n",
    "# I prefer to use iteritems, but to each his own.\n",
    "for col in df.columns:\n",
    "    data.append(  go.Box( y=df[col], name=col, showlegend=False ) )\n",
    "\n",
    "data.append( go.Scatter( x = df.columns, y = df.mean(), mode='lines', name='mean' ) )\n",
    "\n",
    "# This both creates and displays the graph and sends it to and saves it on their server (no non-public data!).\n",
    "py.iplot(data, filename='pandas-box-plot')\n",
    "\n",
    "# cf. https://plot.ly/python/histograms-and-box-plots-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pairwise independence** is not full independence if you can find counterexamples in larger sets.\n",
    "* A **Bernoulli random variable** (or Bernoulli trial) takes on a value of 0 or 1, e.g. success or failure.\n",
    "* A **family of probability distributions** are organized around a **parameter**.\n",
    "* **Discrete CDF**: $F(X) = P(X \\leq x) = \\sum_{y: y \\leq x} p(y)$.\n",
    "* Since CDFs are right-continuous, technically $P(X \\in [a, b]) = F(b) - F(a-)$, where $a-$ is the limit inferior of $a$.\n",
    "* **Discrete expectation**: $E(X) = \\mu_x = \\sum_{x \\in \\Omega} x \\cdot p(x)$.\n",
    "* **Drunk statistician's rule**: $E[h(x)] = \\sum_{x \\in \\Omega} h(x) \\cdot p(x)$.\n",
    "* $\\sigma^2(X) = \\sum_{x \\in \\Omega}(x-\\mu)^2 \\cdot p(x) = E[(X-\\mu)^2] = E(X^2) - [E(X)]^2$\n",
    "* $s$ and $s^2$ in samples correspond with $\\sigma$ and $\\sigma^2$ in distributions.\n",
    "* If $X \\sim bin[n, p](x)$ then $E(X) = np$ and $\\sigma^2(x) = npq$\n",
    "* The **hypergeometric distribution** is the **binomial distribution** either without replacement or without the simplifying assumption that the sample space is so large that the distortion of non-replacement is negligible. It combinatorically corrects for the effects of non-replacement.\n",
    "* Hypergeometric facts, if $X \\sim hyper[N, n_p, n](x)$ ($N$ is population, $n_p$ is successes in the population, $n$ is the draw size) then:\n",
    " * $P(X=x) = \\frac{{n_p \\choose x}{N - n_p \\choose n - x}}{N \\choose n}$\n",
    " * $E(X) = n \\cdot \\frac{n_p}{N}$\n",
    " * $\\sigma^2(X) = \\left(\\frac{N - n}{N - 1}\\right) \\cdot npq$. The fractional term is known as the \"finite population correction term\", is it related to the population sample correction term? Seems suspiciously similar. Also note the mixture of absolute and percentage terms...\n",
    "* The **negative binomial distribution** describes the number of trials necessary before $n$ successes.\n",
    " * $P(X=x) = {x + n - 1 \\choose n - 1}p^n q^x$\n",
    " * $E(X) = \\frac{nq}{p}$\n",
    " * $\\sigma^2(x) = \\frac{nq}{p^2}$\n",
    "* Because the terms of the negative binomial are geometrically convergent it's sometimes refered to as a **geometric sequence**. The other sequence that is loosely refered to this way is the probability distribution for the number of failures before the first success. It's kind of confusing. But the point is that both of these distributions are geometrically convergent (see the $q^x$ term).\n",
    "* The **Poisson distribution** rationalizes the binomial when it is taken to the limit in the case that $n \\to \\infty$ and $p \\to 0$ but $\\lambda = np$ stays constant, then $binom[n,\\:p](x) \\to poisson[\\lambda](x)$.\n",
    "* $\\lambda$ is both mean and variance for a Poisson!\n",
    "* Appendix I from my stochastics textbook has some good computational summaries of the calculations of the characteristics of these functions, including moment generating function calculations, which I'll leave for later.\n",
    "* Interestingly the book goes off then to describe **Poisson arrival processes**, including introducing but not really explaining $o(\\Delta t)$ notation. See my stochastics textbook instead for details.\n",
    "* General rule of thumb for using the Poisson distribution for modeling: $n \\geq 100$, $p \\leq .01$, and $np \\leq 20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypergeometric correction for the binomial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that amongst $n$ balls exactly half are red. What is the probability of, in drawing two balls, getting one red one, if we manipulate the population size?\n",
    "\n",
    "Since we are not replacing the balls the \"correct\" distribution is the hypergeometric one, but as $n \\to \\infty$, $hypergeom[N, \\frac{N}{2}, 2](1) \\to binom[N, \\frac{1}{2}](1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~ResidentMario/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import math\n",
    "\n",
    "binom_probs = [scipy.stats.binom.pmf(1, 2, .5)] * len(list(range(2, 10000, 2))) # 0.5\n",
    "hyper_probs = [scipy.stats.hypergeom.pmf(1, n, n / 2, 2) for n in range(2, 10000, 2)]\n",
    "\n",
    "# Create traces\n",
    "trace1 = go.Scatter(\n",
    "    x = list(range(2, 10000, 2)),\n",
    "    y = binom_probs,\n",
    "    mode = 'lines',\n",
    "    name = 'binomial'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = list(range(2, 10000, 2)),\n",
    "    y = hyper_probs,\n",
    "    mode = 'lines',\n",
    "    name = 'hypergeometric'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        type='log',\n",
    "        autorange=True\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='hypergeometric-approach-to-binomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poisson approximation**\n",
    "\n",
    "Why does the Poisson approximation work? It's a minor coeval to the central limit theorem (still in the to-do). If you explicitly write the binomial distribution out and then insert an approximation involving our $e^{-\\lambda}$ term you will get the result. The math is worked out [here](http://www.stat.yale.edu/~pollard/Courses/241.fall97/Poisson.pdf). I'm not sure if we did this work in stochastics, but we did something similar using Stirling's formula for Poisson arrival processes (that was harder, of course!). I'm satisfied that I get the intrinsic, I think it'll make more sense once I dive into moment-generating functions and revisit stuff with $e$ in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The book then jumps into CDFs and PDFs, all pretty standard except for the inclusion of percentiles!\n",
    "* $\\eta(x)$ is the **percentile function** for a CDF. It's a bijective map, e.g. $\\eta(x): [-\\infty, \\infty] \\to [0, 1]$. \n",
    "* So to solve it just do the $F^{-1}(x)$ inverse transform that we did to transform the uniform random in stochastics.\n",
    "* Drunk statistician's rule is the same for continuous distributions.\n",
    "* The normal distribution edition of the Gaussian, in case you forgot it:\n",
    "$$f[\\mu, \\sigma](x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2 / 2\\sigma^2}$$\n",
    "* What a right beaut.\n",
    "* The CDF for this standard normal is symbolized $\\Phi(z)$, where z is the **z-score** (really the inverse percentile).\n",
    "* The normalization process is $P(X \\leq x) = \\Phi[(x-\\mu)/\\sigma]$\n",
    "* The 68%-85%-99.7% rule-of-thumb for standard deviations.\n",
    "* Any distribution with a large enough number of samples will become approximate a normal distribution (not necessarily a standard normal one). The proof for why is absent in stats textbooks but there's a chapter dedicated to it in my stochastics textbook that we skipped in class which offers a partial proof. A complete proof requires analysis, it says...which I should be able to do after next semester. Ah well.\n",
    "* When approximating a discrete random variable with the continuous normal it is useful to make a **continuity correction**. Since remember we're circumspecting a bunch of boxes with a smooth curve, we're getting an undercount on the left and an overcount on the right. Usual method is to knock .5 off the right and tack it on to the left. This still contains error of couse but it's nearer to the mark, and so is a good general strategy. /\\o/\\\n",
    "* $binom[n, p](x) \\to N(\\mu = np, \\sigma = \\sqrt{npq})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other continuous distributions**\n",
    "\n",
    "* The **gamma distribution** is an important non-symmetric distribution.\n",
    "* $\\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha - 1}e^{-x} dx$\n",
    " * $\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}$\n",
    " * $\\alpha > 1 \\implies \\Gamma(\\alpha) = (\\alpha - 1) \\cdot \\Gamma(\\alpha - 1)$\n",
    " * $n \\in \\mathbb{N} \\implies \\Gamma(n) = (n - 1)!$\n",
    "* $f[\\alpha, \\beta](x) = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha - 1}e^{-x/\\beta}$, $x \\geq 0$.\n",
    "* With $X \\sim \\Gamma[\\alpha, \\beta](x)$, $E(X) = \\alpha\\beta$ and $\\sigma^2 = \\alpha \\beta^2$\n",
    "\n",
    "![A](https://upload.wikimedia.org/wikipedia/commons/e/e6/Gamma_distribution_pdf.svg \"Gamma distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exponential distribution**: $f[\\lambda](x) = \\lambda e^{-\\lambda x}$, $x \\geq 0$, yada yada yada. Note that it's a special case of the gamma distribution.\n",
    "\n",
    "![A](https://upload.wikimedia.org/wikipedia/commons/e/ec/Exponential_pdf.svg \"Exponential distribution\")\n",
    "\n",
    "* The book continues to valiently talk about the Poisson arrival process without doing the math. :) Refer to stochastics.\n",
    "\n",
    "* **Chi-squared distribution**: $f[\\nu](x) = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{(\\nu/2)^{1}e^{-x/2}}$\n",
    "* $\\nu$ is the **number of degrees of freedom**, the chi-squared distribution is important for probability stuff later on involving inference.\n",
    "* The **Weibull distribution** is a sort of less smooth function that kind of operates like the gamma one.\n",
    "* $f[\\alpha, \\beta](x) = \\frac{\\alpha}{\\beta^\\alpha}x^{\\alpha - 1}e^{-(x/\\beta^\\alpha)}$ and then so on.\n",
    "* $\\beta$ is an instance of a **shape parameter**, one which distorts the shape of the distribution. $\\alpha$ is a **scale parameter**, one which acts principally on spread.\n",
    "* In reliability engineering and other applications it is useful for its ability to model failure rates.\n",
    "* The **lognormal** distribution is the $\\ln(\\cdot)$ of a normal distribution. Genius! $f[\\mu, \\sigma] = \\frac{1}{\\sqrt{2\\pi}\\sigma x}e^{-[\\ln(x)-\\mu]^2/(2\\sigma^2)}$\n",
    "* The **beta distribution** is a crazy looking thing that basically gets sheered in between parameters $[A, B]$, that's enough no need to reproduce the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability plotting**\n",
    "* Oh! These are new, and cool. You're given a bunch of data, how do you check to see that the distribution that you have in mind is an appropriate one for modeling the dataset?\n",
    "* A **probability plot** (better termed the **P-P plot**) is a tool for visually checking the fit of your data.\n",
    "* Arrange your $n$ sample observations from smallest to largest, taking each $i$th observation to be the $[100(i-.5)/n]$th sample percentile. In Python code, you create `([dist.pmf([100(i-.5)/n], observations[i]) for i in range(0, n)]`, where n = `len(data)`. Now plot the tuples!\n",
    "* This is a neat application of `plotly`, let's make some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00795722077215\n",
      "0.989185494557\n"
     ]
    }
   ],
   "source": [
    "# The following data is on the compressive strength of concrete (mhm), and the supposition is that it's normal.\n",
    "data = [1400, 1932, 2000, 2200, 2200, 2530, 2630, 2665, 2735, 2735, 2800, 2935, 3000, 3000,\n",
    "        3030, 3065, 3065, 3065, 3170, 3200, 3235, 3260, 3335, 3365, 3465, 3500, 3600, 3600,\n",
    "        3835, 4460]\n",
    "\n",
    "# My (overfitted?) guess at the distribution parameters.\n",
    "dist = scipy.stats.norm(loc=np.mean(data), scale=650)\n",
    "print(dist.cdf(data[0]))\n",
    "print(dist.cdf(data[len(data) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~ResidentMario/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotly import tools\n",
    "\n",
    "# Let's plot it to see if we're right or not!\n",
    "percentiles = [(p - .5)/len(data) for p in range(1, len(data) + 1)]\n",
    "\n",
    "# Create traces\n",
    "trace1 = go.Scatter(\n",
    "    x = list(range(0, len(data))),\n",
    "    y = data,\n",
    "    mode = 'markers',\n",
    "    name = 'raw data'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = percentiles,\n",
    "    y = [dist.cdf(d) for d in data],\n",
    "    mode = 'markers',\n",
    "    name = 'normal plot'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = percentiles,\n",
    "    y = percentiles,\n",
    "    mode = 'line',\n",
    "    name = 'p = p'\n",
    ")\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 1, 2)\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(fig, filename='normal-pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* The **Q-Q plot** is more commonly used for this, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~ResidentMario/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles = [(p - .5)/len(data) for p in range(1, len(data) + 1)]\n",
    "\n",
    "# Create traces\n",
    "trace1 = go.Scatter(\n",
    "    x = list(range(0, len(data))),\n",
    "    y = data,\n",
    "    mode = 'markers',\n",
    "    name = 'raw data'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = [dist.ppf(p) for p in percentiles],\n",
    "    y = data,\n",
    "    mode = 'markers',\n",
    "    name = 'q - q'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = [dist.ppf(p) for p in percentiles],\n",
    "    y = [dist.ppf(p) for p in percentiles],\n",
    "    mode = 'line',\n",
    "    name = 'q = q'\n",
    ")\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 1, 2)\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(fig, filename='normal-qq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A population sample which is non-normal is usually one of:\n",
    " * Symettric with lighter-than-Gaussian tails.\n",
    " * Symmetric with heavier-than-Gaussian tails.\n",
    " * Skewed.\n",
    "* You can read a lighter-tailed or heavier-tailed distribution off of the against-the-normal Q-Q plot. A lighter-tailed distribution will be s-curved towards the observed axis, a heavier-tailed distribution will be s-curved towards the actual axis.\n",
    "* The book gives 30 as a rule of thumb for when large deviations from the standard normal pattern are indicative of the distribution not actually being normal.\n",
    "* You can also Q-Q plot a Weibull distribution, apparently in that case plotting $[\\eta(x), ln(x)]$ works well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint and multivariate probabilities**\n",
    "\n",
    "* $P[(X,Y) \\in A] = \\sum_{x \\in X}\\sum_{y \\in Y}f(x,y)$ (discrete)\n",
    "* $p_X(x) = \\sum_{y \\in Y} f(x,y)$\n",
    "* $P[(X, Y) \\in A] = \\iint_A f(x,y)dx dy$ (continuous)\n",
    "* $f_X(x) = \\int_{-\\infty}^\\infty f(x, y) dy$\n",
    "* $f_Y(y) = \\int_{-\\infty}^\\infty f(x, y) dx$\n",
    "* $X \\bot Y \\leftrightarrow f(x,y) = f_X(x)f_Y(y)$\n",
    "* $f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)}$\n",
    "* Drunk statistician's rule holds in the multivariate case.\n",
    "* $Cov(X,Y) = E[(X-E[X])(Y-E[Y])]$\n",
    " * $\\sum_{x \\in X} \\sum_{y \\in Y} (x - E[X]) (y - E[Y]) \\cdot f_{X,Y}(x,y)$, discrete\n",
    " * $\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty (x - E[X])(y - E[Y])f_{X,Y}(x,y)dx dy$, continuous\n",
    "* Covariance works because if $X$ and $Y$ are correlated, their multiples will be $(+)(+)$ or $(-)(-)$; otherwise they will be $(+)(-)$ or $(-)(+)$ indicating negative correlation.\n",
    "* Zero or near-zero covariance (when variables are **uncorrelated**) is not indicative of independence, just of this particular relationship canceling itself out.\n",
    "* Shortcut we used all the time in stochastic, $Cov(X,Y) = E[XY] - E[X]E[Y]$\n",
    "* $\\rho_{X,Y} = \\frac{Cov(X,Y)}{\\sigma_X \\cdot \\sigma_y}$\n",
    "* Covariance depends on the unit of measurement, correlation is not. Thus correlation standardizes covariance measure to $[-1, 1]$, which is necessary for comparative purposes.\n",
    "* Independent random variables have a covariance/correlation of 0, but the latter does not imply the former (as stated above).\n",
    "* Interestingly, $\\rho = 1$ or $\\rho = -1$ iff $Y$ is a linear transform of $X$, e.g. $Y = aX + b$.\n",
    "* The book defines **idd rvs**. :)\n",
    "* The expectation of a sum is the sum of the expectations.\n",
    "* The variance of a sum is operative across only when the random variables are uncorrelated (a weaker condition than independence).\n",
    "* At this point the book veers into avoiding moment generating functions. This would be a good time to learn these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
